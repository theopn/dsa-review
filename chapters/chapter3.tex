%! TEX root = ../dsa-review.tex

Once you store all the items in a data structure, you might want to organize them for the future use (such as selecting nth largest element). For this, you have to \textit{sort} the data structure (in this book, array will be assumed). \textit{Sorting} is deciding how to permute the array elements until they are sorted.

There are couple aspects of sorting algorithms you need to consider:

\begin{itemize}
  \item Runtime: When analyzing a runtime of a sorting algorithm, both number of compares and number of swaps are considered. \textbf{Most sorting algorithms make more comparisons than swaps}, but if a sorting algorithm makes more swaps, it must be used for the asymptotic runtime analysis
  \item Stability: An algorithm is stable if it preserves the input ordering of equal items.
    \begin{center}
      \begin{tikzpicture}
        \node [left] at (-1, 1) {unsorted list};
        \node [draw, minimum width=1cm, fill=gray] at (0, 1) {5};
        \node [draw, minimum width=1cm] at (1, 1) {3};
        \node [draw, minimum width=1cm, fill=yellow] at (2, 1) {5'};
        \node [draw, minimum width=1cm] at (3, 1) {2};
        \node [draw, minimum width=1cm] at (4, 1) {7};

        \node [left] at (-1, 0) {sorted with stable sorting algorithm};
        \node [draw, minimum width=1cm] at (0, 0) {2};
        \node [draw, minimum width=1cm] at (1, 0) {3};
        \node [draw, minimum width=1cm, fill=gray] at (2, 0) {5};
        \node [draw, minimum width=1cm, fill=yellow] at (3, 0) {5'};
        \node [draw, minimum width=1cm] at (4, 0) {7};

        \node [left] at (-1, -1) {sorted with unstable sorting algorithm};
        \node [draw, minimum width=1cm] at (0, -1) {2};
        \node [draw, minimum width=1cm] at (1, -1) {3};
        \node [draw, minimum width=1cm, fill=yellow] at (2, -1) {5'};
        \node [draw, minimum width=1cm, fill=gray] at (3, -1) {5};
        \node [draw, minimum width=1cm] at (4, -1) {7};
      \end{tikzpicture}
    \end{center}
  \item In-place: An algorithm is in-place if it can directly sorts the items without making a copy or extra array(s)
\end{itemize}

\section{Bubble, Selection, Insertion Sort}

The first three sorting algorithms we will discuss are \textproc{Bubble-sort}, \textproc{Selection-sort}, and \textproc{Insertion-sort}. They are simple to understand and implement, however, they all have the worst-case runtime of $O(n^{2})$, which limits their usages.

\textproc{Bubble-sort} goes through the array and swap elements that are out of order, and if such element is found, it re-starts from the beginning of the list.

\begin{python}
def bubble_sort(arr):
    n = len(arr)
    repeat = True
    while repeat:
        repeat = False

        for i in range(n - 1):
            if arr[i] > arr[i + 1]:
                arr[i], arr[i + 1] = arr[i + 1], arr[i]
                repeat = True

    return arr
\end{python}

\begin{center}
  \begin{tabular}{ | c | c | }
    \hline
    In-place? & Stable? \\
    \hline
    True & True \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | c | c | c | }
    \hline
    - & NumCompares & NumSwaps \\
    \hline
    Already Sorted & $n - 1$ & $0$ \\
    \hline
    Worst Case & $n^{2} - n$ & $\frac{1}{2} n^{2} - \frac{1}{2} n$ \\
    \hline
  \end{tabular}
\end{center}

\textproc{Selection-sort} is a sorting algorithm closest to our ``natural'' thought of sorting an array. It finds the smallest, second smallest, ... elements and place them accordingly. It makes the same number of comparisons in any cases.

\begin{python}
def selection_sort(arr):
    n = len(arr)
    for i in range(n - 1):
        min_idx = i

        for j in range(i + 1, n):
            if arr[j] < arr[min_idx]:
                min_idx = j

        if i != min_idx:
            arr[i], arr[min_idx] = arr[min_idx], arr[i]

    return arr
\end{python}

\begin{center}
  \begin{tabular}{ | c | c | }
    \hline
    In-place? & Stable? \\
    \hline
    True & False \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | c | c | c | }
    \hline
    - & NumCompares & NumSwaps \\
    \hline
    Already Sorted & $\frac{1}{2} n^2 - \frac{1}{2} n$ & $0$ \\
    \hline
    Worst Case &  $\frac{1}{2} n^2 - \frac{1}{2} n$ & $\lfloor \frac{1}{2} n \rfloor$ \\
    \hline
  \end{tabular}
\end{center}

\textproc{Insertion-sort} takes one element at a time and places it in the correct index of the partially sorted sub-array until the array is sorted.

\begin{python}
def insertion_sort(arr):
    n = len(arr)

    for i in range(1, n):
        j = i - 1

        while j >= 0 and arr[j] > arr[j + 1]:
            arr[j], arr[j + 1] = arr[j + 1], arr[j]
            j -= 1

    return arr
\end{python}

\begin{center}
  \begin{tabular}{ | c | c | }
    \hline
    In-place? & Stable? \\
    \hline
    True & True \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | c | c | c | }
    \hline
    - & NumCompares & NumSwaps \\
    \hline
    Already Sorted & $n - 1$ & $0$ \\
    \hline
    Worst Case &  $\frac{1}{2} n^2 - \frac{1}{2} n$ & $\frac{1}{2} n^2 - \frac{1}{2} n$ \\
    \hline
  \end{tabular}
\end{center}

\section{Shell Sort}

\textproc{Shell-sort} runs insertion sort with ``gaps'', the index margin where insertion sort will be performed.

\noindent \hrulefill
\begin{algorithmic}[1]
  \Function{Shell-sort}{$A, H$} \Comment{$A$ is an array size $n$, $H$ is the array size $m$ containing gap values}
    \For{$h = 0$ to $m - 1$}
      \State $gap \gets H[h]$
      \For{$i = 1$ to $n - 1$}
        \State $j \gets i - 1$
        \While{$j \geq 0$ and $j + gap < n$}
          \If{$A[j] > A[j + gap]$}
            \State \Call{Swap}{$A, j, j + gap$}
            \State $j \gets j - gap$
          \EndIf
        \EndWhile
      \EndFor
    \EndFor
    \item[]
    \State \Return{$A$}
  \EndFunction
\end{algorithmic}
\noindent \hrulefill

\section{Heap Sort}

\textproc{Heap-sort} uses binary max-heap to sort an array. While it's the first sorting algorithm to utilize a data structure, it's not preferred in real life due to cache issue.

\begin{python}
def heap_sort(arr):
    n = len(arr)

    build_heap(arr)

    def sortdown(i):
        # move the first element of the heap (largest) to the back
        arr[i], arr[0] = arr[0], arr[i]
        # rebuild heap with the first i elements
        max_heapify(arr, i, 0)

    for i in range(n - 1, 0, -1):
        sortdown(i)

    return arr
\end{python}

The algorithm first builds the heap from the array elements (refer to section 2.6.1 for the \textproc{max\_heapify} and \textproc{build\_heap} functions). Then the algorithm calls \textproc{Sort-down} from the last heap elements down to the first.

\subsection{Sort Down Algorithm}

\begin{python}
    def sortdown(i):
        # move the first element of the heap (largest) to the back
        arr[i], arr[0] = arr[0], arr[i]
        # rebuild heap with the first i elements
        max_heapify(arr, i, 0)
\end{python}

Suppose we have a max-heap $A = [ 7, 6, 3, 5, 4, 1]$. Using the property of max-heap that the largest element is always placed in the first index, \textproc{Heap-sort} performs \textproc{Sort-down} $n - 1$ times, which places the last element to a correct position and rebuild the heap on the last of the element.

One can call \textproc{Swim-up} instead of \textproc{Swim-down} to repair the heap. However, the former takes $O(n \log n)$ time and the latter takes $O(n)$ time.

\begin{enumerate}
  \item $i = 5$: \\
    \textproc{swap($A, 5, 0$)}: [ 1, 6, 3, 5, 4, \st{7} ] \\
    \textproc{swim-down}{($5 - 1$)} (rebuild heap from index 0 to 4): [ 6, 5, 3, 1, 4 \st{7}]
  \item $i = 4$: \\
    \textproc{swap($A, 4, 0$)}: [ 4, 5, 3, 1, \st{6}, \st{7} ] \\
    \textproc{swim-down}{($4 - 1$)}: [ 5, 4, 3, 1, \st{6}, \st{7}]
  \item $i = 3$: \\
    \textproc{swap($A, 3, 0$)}: [ 1, 4, 3, \st{5}, \st{6}, \st{7} ] \\
    \textproc{swim-down}{($3 - 1$)}: [ 4, 1, 3, \st{5}, \st{6}, \st{7}]
  \item $i = 2$: \\
    \textproc{swap($A, 2, 0$)}: [ 3, 1, \st{4}, \st{5}, \st{6}, \st{7} ] \\
    \textproc{swim-down}{($2 - 1$)}: [ 3, 1, \st{4}, \st{5}, \st{6}, \st{7}]
  \item $i = 1$: \\
    \textproc{swap($a, 1, 0$)}: [ 1, \st{3}, \st{4}, \st{5}, \st{6}, \st{7} ] \\
    \textproc{swim-down}{($1 - 1$)}: [ 1, \st{3}, \st{4}, \st{5}, \st{6}, \st{7}]
\end{enumerate}

\section{Merge Sort}

\textproc{Merge-sort} is a \textit{divide-and-conquer} sorting algorithm that divides the array down to multiple lists with one element and merge them together.

\begin{python}
def merge(arr, l, m, r):
    n1 = m - l + 1
    n2 = r - m
    L = [None] * (n1 + 1)
    R = [None] * (n2 + 1)

    # arrssign elements to each array
    for i in range(n1):
        L[i] = arr[l + i]
    for j in range(n2):
        R[j] = arr[m + j + 1]

    L[n1], R[n2] = float("inf"), float("inf")
    i, j = 0, 0

    for k in range(l, r + 1):
        if L[i] <= R[j]:
            arr[k] = L[i]
            i += 1
        else:
            arr[k] = R[j]
            j += 1

    return arr
\end{python}

The algorithm recursively calls itself with half of the current array, and once all the sub-arrays are of size 1, it begins merging them together. The diagram below shows how \textproc{Merge-sort} breaks $A = [3, 5, 2, 1, 4, 7, 6]$ down.

\begin{center}
  \begin{forest}
    for tree={edge={gray!70, very thick, -latex}, draw}, % -latex == ->
    [{ $[3, 5, 2, 1, 4, 7, 6]$ } % lv 0
      %
      [{ $[3, 5, 2, 1]$ }
        %
        [{ $[3, 5]$ }
          [{ $[3]$ }]
          [{ $[5]$ }]
        ]
        %
        [{ $[2, 1]$ }
          [{ $[2]$ }]
          [{ $[1]$ }]
        ]
      ]
      %
      [{ $[4, 7, 6]$ }
        %
        [{ $[4, 7]$ }
          [{ $[4]$ }]
          [{ $[7]$ }]
        ]
        %
        [{ $[6]$ }
        ]
      ]
    ]
  \end{forest}
\end{center}

\begin{python}
def merge_sort(arr, l, r):
    if l < r:
        m = (l + r) // 2
        merge_sort(arr, l, m)
        merge_sort(arr, m + 1, r)
        merge(arr, l, m, r)
    return arr
\end{python}

\textproc{Merge} algorithm merges two sorted arrays by ``climbing the ladder''. The diagram on the right shows how \textproc{Merge} algorithm merges the array we split earlier in the recursive step of the \textproc{Merge-sort}, and the diagram on the left shows the visualization of how \textproc{Merge} algorithm merges two sorted array by ``climbing the ladder.''

\begin{multicols}{2}
  \begin{forest}
    for tree={grow'=north, edge={gray!70, very thick, latex-}, draw}, % latex-== <-
    [{ $[1, 2, 3, 4, 5, 6, 7]$ } % lv 0
      %
      [{ $[1, 2, 3, 5]$ }
        %
        [{ $[3, 5]$ }
          [{ $[3]$ }]
          [{ $[5]$ }]
        ]
        %
        [{ $[1, 2]$ }
          [{ $[2]$ }]
          [{ $[1]$ }]
        ]
      ]
      %
      [{ $[4, 6, 7]$ }
        %
        [{ $[4, 7]$ }
          [{ $[4]$ }]
          [{ $[7]$ }]
        ]
        %
        [{ $[6]$ }
        ]
      ]
    ]
  \end{forest}

  \begin{tikzpicture}
    \foreach \x [ evaluate={\l=int(\x + 1)}; ] in {0,...,6} {
      \node [draw, minimum width=1cm] at (\x, 1) {\l};
    } % foreach
    \node at (0, 0.3) {$\underline{1} : 4$};
    \node at (1, 0.3) {$\underline{2} : 4$};
    \node at (2, 0.3) {$\underline{3} : 4$};
    \node at (3, 0.3) {$5 : \underline{4}$};
    \node at (4, 0.3) {$\underline{5} : 6$};
    \node at (5, 0.3) {$\infty : \underline{6}$};
    \node at (6, 0.3) {$\infty : \underline{7}$};
  \end{tikzpicture}
\end{multicols}

\begin{enumerate}
  \item Sub-arrays $L$ and $R$ are sorted, and their last element are set to $\infty$. In this example, $L = [1, 2, 3, 5, \infty], R = [4, 6, 7, \infty]$
  \item $k = 0, i = 0, j = 0$: Since $L[0] = 1 < 4 = R[0]$, place $L[0]$ in $A[0]$ and increment $i$
  \item $k = 1, i = 1, j = 0$: Since $L[1] = 2 < 4 = R[0]$, place $L[1]$ in $A[1]$ and increment $i$
  \item $k = 2, i = 2, j = 0$: Since $L[2] = 3 < 4 = R[0]$, place $L[2]$ in $A[2]$ and increment $i$
  \item $k = 3, i = 3, j = 0$: Since $L[3] = 5 > 4 = R[0]$, place $R[0]$ in $A[3]$ and increment $j$
  \item $k = 4, i = 3, j = 1$: Since $L[3] = 5 < 6 = R[1]$, place $L[3]$ in $A[4]$ and increment $i$
  \item $k = 5, i = 4, j = 1$: Since $L[4] = \infty > 6 = R[1]$, place $R[1]$ in $A[5]$ and increment $j$
  \item Because the sub-array $L$ reached its $\infty$ element, I will skip the rest of the steps and place rest of the elements in $R$ to $A$
\end{enumerate}

\section{Quick Sort}

\textproc{Quick-sort} is another divide-and-conquer sorting algorithm.

\begin{python}
def quick_sort(arr, l, r):
    if l < r:
        m = partition(arr, l, r)
        quick_sort(arr, l, m - 1)
        quick_sort(arr, m + 1, r)
    return arr
\end{python}

\subsection{Partition and Pivot}

\begin{python}
def partition(arr, l, r):
    p = arr[r]  # rightmost element as the pivot
    i = l - 1
    for j in range(l, r + 1):
        if arr[j] < p:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]
    i += 1
    arr[i], arr[r] = arr[r], arr[i]
    return i
\end{python}

\section[Comparison Sorting Algo Lower Bound]{Decision Tree and the Lower Bound for Comparison Sorting Algorithm }

So far, we have been discussing \textit{comparison sorting algorithms} (sorting algorithms that only reads array elements through $>, =, <$ comparison). We can draw a decision tree with all the permutations of how a comparison sorting algorithms would compare and sort the array $[a_{0}, a_{1}, a_{2}]$.

\begin{center}
  \begin{forest}
    for tree={
      delay={
        edge label/.wrap value={node[midway, above, font=\scriptsize]{#1}},
      },
    }
    [{ $a_{0}, a_{1}$ } % lv 0
      %
      [{ $a_{1}, a_{2}$ }, edge label={$\leq$} % lv 0
        [{ $[a_{0}, a_{1}, a_{2}]$ }, edge label={$\leq$}] % lv 2
        [{ $a_{0}, a_{2}$ }, edge label={$>$} % lv 1
          [{ $[a_{0}, a_{2}, a_{1}]$ }, edge label={$\leq$}] % lv 3
          [{ $[a_{2}, a_{0}, a_{1}]$ }, edge label={$>$}] % lv 3
        ]
      ]
      %
      [{ $a_{0}, a_{2}$ }, edge label={$>$} % lv 1
        [{ $[a_{1}, a_{0}, a_{2}]$ }, edge label={$\leq$}] % lv 2
        [{ $a_{1}, a_{2}$ }, edge label={$>$} % lv 2
          [{ $[a_{1}, a_{2}, a_{0}]$ }, edge label={$\leq$}] % lv 3
          [{ $[a_{2}, a_{1}, a_{0}]$ }, edge label={$>$}] % lv 3
        ]
      ]
    ]
  \end{forest}
\end{center}

The decision tree for array size of 3 has $3! = 6$ leaves, and it is trivial that a decision tree for an \textbf{array with $n$ elements will have $n!$ leaves}. The height of the decision tree represents the worst-case number of comparisons the algorithm has to make in order to sort the array.

A tree with the height $h$ has at most $2^{h}$ leaves. Using this property, we can have the lower Big-omega bound for height of the decision tree for comparison sorting algorithms.

\begin{align*}
  2^{h} \geq n! \therefore h \geq \log_{2}(n!) \tag{$h$ is the height of the decision tree} \\
  h \geq \log_{2}(n!) = \log_{2}(1 \cdot 2 \cdot \cdots \cdot (n - 1) \cdot n) \\
  = \log_{2}(1) + \log_{2}(2) + \cdots + \log_{2}(n - 1) + \log_{2}(n) \\
  = \sum_{i = 1}^{n} \log_{2}(i) = \sum_{i = 1}^{\frac{n}{2} - 1} \log_{2}(i) + \sum_{i = \frac{n}{2}}^{n} \log_{2}(i) \\
  \geq 0 + \sum_{i = \frac{n}{2}}^{n} \log_{2}(i) \geq 0 + \sum_{i = \frac{n}{2}}^{n} \log_{2}(\frac{n}{2}) \tag{$i$ in the former expression is $\geq \frac{n}{2}$} \\
  = \frac{n}{2} \log_{2}(\frac{n}{2}) \tag{$i = \frac{n}{2}$ to $n$ is exactly $\frac{n}{2}$ iterations} \\
  \in \Theta(n \log_{2}(n))
\end{align*}

Because $h \geq \log_{2}(n!) \in \Theta(n \log_{2}(n))$, $h \in \Omega(n \log_{2}(n))$. Therefore, we can conclude that any \textbf{comparison sorting algorithms cannot run faster than $O(n \log_{2}(n))$ time} in the worst-case scenario.

\section{Bucket Sort}

\textproc{Bucket-sort} is a sorting algorithm where elements are divided into each "bucket" and a different sorting algorithm is called for each bucket. While \textproc{Bucket-sort} is a comparison sorting algorithm, it's an attempt to reduce the runtime by decreasing the number of elements that the sorting algorithm has to sort.

\section{Counting Sort}

\textproc{Counting-sort} is a \textit{non-comparison} sorting algorithm. It uses the extra array $count$, where its index initially represents the value of each element in $A$ (e.g., if there are three 5's in $A$, $count[5] = 3$ before the ``accumulation'' step to determine the final index), to sort the array.

\begin{python}
def countingSort(arr):
    n = len(arr)

    # Step 1 O(n): Find the maximum element and initialize count array
    k = max(arr)
    cnt = [0] * (k + 1)

    # Step 2 O(n): Find the number of occurences in each element in the array
    for i in range(n):
        cnt[arr[i]] += 1

    # Step 3 O(k): Convert count into a prefix sum array of itself
    for i in range(1, k + 1):
        cnt[i] += cnt[i - 1]

    # Step 4 O(n): Use the count array to find the index of each element
    out = [None] * n
    for i in range(n - 1, -1, -1):
        out[cnt[arr[i]] - 1] = arr[i]
        cnt[arr[i]] -= 1

    return out
\end{python}

\begin{enumerate}
  \item Suppose we have an array $A = [ 2, 5, 3, 0, 2, 3, 0, 3 ]$. $k = \textproc{max}(A) = 5$.
  \item Initialize $count$, the array size $5 + 1$, with 0's. $count = [0, 0, 0, 0, 0, 0]$.
  \item Count number of occurrence. $count = [2, 0, 2, 3, 0, 1]$ (e.g., 2 occurred 2 times)
  \item Accumulate values of $count$ from left to right. $count = [2, 2, 4, 7, 7, 8]$ (e.g., $count[1] = 2 + 0, count[2] = 2 + 0 + 2, \ldots$)
  \item Initialize $out$, the array size $n = 8$, with nil's. $out = [nil, nil, nil, nil, nil, nil, nil, nil]$.
  \item Place each element to the $out$ array using $count$ array
    \begin{enumerate}
      \item When $i = n - 1 = 7$: $A[7] = 3 \text{ and } count[3] = 7 \Rightarrow out[7 - 1] := A[7] = 3 \text{ and } count[3] := 7 - 1$
        \begin{center}
          out = [nil, nil, nil, nil, nil, nil, 3, nil] \\
          count = [2, 2, 4, 6, 7, 8]
        \end{center}
      \item When $i = n - 2 = 6$: $A[6] = 0 \text{ and } count[0] = 2 \Rightarrow out[2 - 1] := A[6] = 0 \text{ and } count[0] := 2 - 1$
        \begin{center}
          out = [nil, 0, nil, nil, nil, nil, 3, nil] \\
          count = [1, 2, 4, 6, 7, 8]
        \end{center}
      \item When $i = n - 3 = 5$: $A[5] = 3 \text{ and } count[3] = 6 \Rightarrow out[6 - 1] := A[5] = 3 \text{ and } count[3] := 6 - 1$
        \begin{center}
          out = [nil, 0, nil, nil, nil, 3, 3, nil] \\
          count = [1, 2, 4, 5, 7, 8]
        \end{center}
      \item $\ldots$
        \begin{center}
          out = [0, 0, 2, 2, 3, 3, 3, 5] \\
          count = [0, 2, 2, 4, 7, 7]
        \end{center}
    \end{enumerate}
\end{enumerate}


\begin{center}
  \begin{tabular}{ | c | c | }
    \hline
    In-place? & Stable? \\
    \hline
    False & True \\
    \hline
  \end{tabular}
\end{center}

Because of its use for \textproc{Radix-sort}, \textproc{Counting-sort} must be stable, and it indeed is. If there are items with the same value, it will be moved to the $out$ array in order in the last (third) for loop.

\begin{center}
  \begin{tabular}{ | c | c | }
    \hline
    Runtime & Space Usage \\
    \hline
    $O(n + k)$ & $O(n + k)$ \\
    \hline
  \end{tabular}
\end{center}

As the algorithm iterates both the size of the array $n$ and the maximum element in the array $k$, \textbf{the algorithm runs in $O(n + k)$ time and uses $O(n + k)$ space}.

\section{Radix Sort}

\textproc{Radix-sort} is a non-comparative sorting algorithm for elements with more than one significant digits. It utilizes a stable sorting algorithm such as \textproc{Counting-sort} to sort elements lexicographically.

\noindent \hrulefill
\begin{algorithmic}[1]
  \Function{Radix-sort}{$A, k$} \Comment{$A$ is an array where the maximum dimension of an element is $d$}
    \For{$i = d$ down to $1$}
      \State Call a \textbf{stable} sorting algorithm at dimension $i$
    \EndFor
    \State \Return{$A$}
  \EndFunction
\end{algorithmic}
\noindent \hrulefill

\subsection{Lexicographic Order}

\[
  (x_1, x_2, \ldots, x_d) < (y_1, y_2, \ldots, y_d) \\
  \Leftrightarrow \\
  (x_i < y_i) \lor (x_1 = y_1 \land (x_2, \ldots, x_d) < (y_2, \ldots, y_d))
\]

